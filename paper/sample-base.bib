%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Calvin Walker at 2024-12-09 10:58:12 -0600 


%% Saved with string encoding Unicode (UTF-8) 


@string{academic = {Academic Press}}

@string{acmmathsoft = {{ACM} Trans. Math. Software}}

@string{acmpress = {{ACM} Press}}

@string{adamhilger = {Adam Hilger}}

@string{addisonwesley = {Addison-Wesley}}

@string{allynbacon = {Allyn and Bacon}}

@string{amermathmonthly = {Amer. Math. Monthly}}

@string{amersocio = {American Journal of Sociology}}

@string{amerstatassoc = {J. Amer. Statist. Assoc.}}

@string{ams = {American Mathematical Society}}

@string{amstrans = {Amer. Math. Soc. Transl.}}

@string{applmathcomp = {Appl. Math. Comput.}}

@string{birkhauser = {Birkha{\"u}ser}}

@string{bit = {{BIT}}}

@string{britstatpsych = {Brit. J. Math. Statist. Psych.}}

@string{bullams = {Bull. Amer. Math. Soc.}}

@string{cacm = {Commun. {ACM}}}

@string{cambridgepress = {Cambridge University Press}}

@string{canmathbull = {Canad. Math. Bull.}}

@string{chelsea = {Chelsea}}

@string{claredonpress = {Claredon Press}}

@string{compapplmath = {J. Comput. Appl. Math.}}

@string{compjour = {Comput. J.}}

@string{compphys = {J. Comput. Phys.}}

@string{compserv = {Comput. Surveys}}

@string{compstruct = {Comput. \& Structures}}

@string{compsyssci = {J. Comput. System Sci.}}

@string{computer = {{IEEE} Computer}}

@string{computing = {Computing}}

@string{contempmath = {Contemp. Math.}}

@string{crelle = {Crelle's Journal}}

@string{doverpub = {Dover Publications}}

@string{eyolles = {Eyolles}}

@string{giornalemath = {Giorn. Mat.}}

@string{holtrinehartwinston = {Holt, Rinehart and Winston}}

@string{ieeespec = {{IEEE} Spectrum}}

@string{ieeetransac = {{IEEE} Trans. Automat. Control}}

@string{ieeetransaeroelec = {{IEEE} Trans. Aerospace Electron. Systems}}

@string{ieeetranscomp = {{IEEE} Trans. Comput.}}

@string{imanumerana = {{IMA} J. Numer. Anal.}}

@string{infproclet = {Inform. Process. Lett.}}

@string{instmathapp = {J. Inst. Math. Appl.}}

@string{intcontrol = {Internat. J. Control}}

@string{interscience = {Interscience}}

@string{intnumereng = {Internat. J. Numer. Methods Engrg.}}

@string{intsuper = {Internat. J. Supercomputing Applic.}}

@string{jacm = {J. ACM}}

@string{johnshopkinspress = {The Johns Hopkins University Press}}

@string{johnwileysons = {John Wiley and Sons}}

@string{jresnatburstand = {J. Res. Nat. Bur. Standards}}

@string{jsiam = {J. Soc. Indust. Appl. Math.}}

@string{jsiamb = {J. Soc. Indust. Appl. Math. Ser. B Numer. Anal.}}

@string{kibernetika = {Kibernetika}}

@string{linalgapp = {Linear Algebra Appl.}}

@string{macmillan = {Macmillan}}

@string{mathanaappl = {J. Math. Anal. Appl.}}

@string{mathannalen = {Math. Ann.}}

@string{mathcomp = {Math. Comp.}}

@string{mathphys = {J. Math. Phys.}}

@string{mathscand = {Math. Scand.}}

@string{mathworks = {The Math Works Inc.}}

@string{mcgrawhill = {McGraw-Hill}}

@string{natburstd = {National Bureau of Standards}}

@string{northholland = {North-Holland}}

@string{numermath = {Numer. Math.}}

@string{oxfordpress = {Oxford University Press}}

@string{pacificmath = {Pacific J. Math.}}

@string{parcomputing = {Parallel Comput.}}

@string{pardistcomp = {J. Parallel and Distrib. Comput.}}

@string{pergamonpress = {Pergamon Press}}

@string{philmag = {Philos. Mag.}}

@string{plenumpress = {Plenum Press}}

@string{prenticehall = {Prentice-Hall}}

@string{procams = {Proc. Amer. Math. Soc.}}

@string{procieee = {Proc. {IEEE}}}

@string{procnas = {Proc. Nat. Acad. Sci. U. S. A.}}

@string{psychometrika = {Psychometrika}}

@string{quartapplmath = {Quart. Appl. Math.}}

@string{quartmath = {Quart. J. Math. Oxford Ser. (2)}}

@string{revueinststat = {Rev. Inst. Internat. Statist.}}

@string{siamalgmeth = {{SIAM} J. Algebraic Discrete Methods}}

@string{siamappmath = {{SIAM} J. Appl. Math.}}

@string{siamcomp = {{SIAM} J. Comput.}}

@string{siammatrix = {{SIAM} J. Matrix Anal. Appl.}}

@string{siamnumanal = {{SIAM} J. Numer. Anal.}}

@string{siampub = {{SIAM} Publications}}

@string{siamreview = {{SIAM} Rev.}}

@string{siamscistat = {{SIAM} J. Sci. Statist. Comput.}}

@string{signum = {{ACM} {SIGNUM} Newslett.}}

@string{softpracexp = {Software Prac. Experience}}

@string{springer = {Springer-Verlag}}

@string{statscience = {Statist. Sci.}}

@string{tablesaidscomp = {Math. Tables Aids Comput.}}

@string{techno = {Technometrics}}

@string{texaspress = {University of Texas Press}}

@string{transams = {Trans. Amer. Math. Soc.}}

@string{ussrcompmathphys = {{U. S. S. R.} Comput. Math. and Math. Phys.}}

@string{vannostrand = {Van Nostrand}}

@string{vlsicompsys = {J. {VLSI} Comput. Syst.}}

@string{whfreeman = {W. H. Freeman and Co.}}

@string{zangewmathmech = {Z. Angew. Math. Mech.}}

@string{zangewmathphys = {Z. Angew. Math. Phys.}}


@misc{math-verifiers,
	abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
	author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
	date-added = {2024-12-09 10:57:19 -0600},
	date-modified = {2024-12-09 10:58:12 -0600},
	doi = {10.48550/arXiv.2110.14168},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	month = nov,
	note = {arXiv:2110.14168},
	publisher = {arXiv},
	title = {Training {Verifiers} to {Solve} {Math} {Word} {Problems}},
	url = {http://arxiv.org/abs/2110.14168},
	urldate = {2024-12-09},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2110.14168},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2110.14168}}

@misc{lora,
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	date-added = {2024-12-09 10:57:18 -0600},
	date-modified = {2024-12-09 10:58:06 -0600},
	doi = {10.48550/arXiv.2106.09685},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	month = oct,
	note = {arXiv:2106.09685},
	publisher = {arXiv},
	shorttitle = {{LoRA}},
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2106.09685},
	urldate = {2024-12-09},
	year = {2021},
	bdsk-url-1 = {http://arxiv.org/abs/2106.09685},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2106.09685}}

@misc{hypothesis-search,
	abstract = {Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves 30\% accuracy, outperforming the direct prompting baseline (accuracy of 17\%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33\%. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks.},
	author = {Wang, Ruocheng and Zelikman, Eric and Poesia, Gabriel and Pu, Yewen and Haber, Nick and Goodman, Noah D.},
	date-added = {2024-12-09 10:57:16 -0600},
	date-modified = {2024-12-09 10:58:01 -0600},
	doi = {10.48550/arXiv.2309.05660},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	month = may,
	note = {arXiv:2309.05660},
	publisher = {arXiv},
	shorttitle = {Hypothesis {Search}},
	title = {Hypothesis {Search}: {Inductive} {Reasoning} with {Language} {Models}},
	url = {http://arxiv.org/abs/2309.05660},
	urldate = {2024-12-09},
	year = {2024},
	bdsk-url-1 = {http://arxiv.org/abs/2309.05660},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2309.05660}}

@misc{easy-to-hard,
	abstract = {Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as {\textbackslash}textit\{easy-to-hard generalization\}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervised reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such {\textbackslash}textit\{easy-to-hard generalization from evaluators\} can enable {\textbackslash}textit\{easy-to-hard generalizations in generators\} either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model achieves an accuracy of 34.0{\textbackslash}\% on MATH500, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.},
	author = {Sun, Zhiqing and Yu, Longhui and Shen, Yikang and Liu, Weiyang and Yang, Yiming and Welleck, Sean and Gan, Chuang},
	date-added = {2024-12-09 10:57:13 -0600},
	date-modified = {2024-12-09 10:57:55 -0600},
	doi = {10.48550/arXiv.2403.09472},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	month = mar,
	note = {arXiv:2403.09472},
	publisher = {arXiv},
	shorttitle = {Easy-to-{Hard} {Generalization}},
	title = {Easy-to-{Hard} {Generalization}: {Scalable} {Alignment} {Beyond} {Human} {Supervision}},
	url = {http://arxiv.org/abs/2403.09472},
	urldate = {2024-12-09},
	year = {2024},
	bdsk-url-1 = {http://arxiv.org/abs/2403.09472},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2403.09472}}

@misc{arc-prize,
	abstract = {Learn more about the only AI benchmark that measures AGI progress.},
	date-added = {2024-12-09 10:57:11 -0600},
	date-modified = {2024-12-09 10:57:49 -0600},
	journal = {ARC Prize},
	language = {en},
	title = {{ARC} {Prize} - {What} is {ARC}-{AGI}?},
	url = {https://arcprize.org/arc},
	urldate = {2024-12-09},
	bdsk-url-1 = {https://arcprize.org/arc}}
